{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "sys.path.append(os.path.abspath(os.path.join('../utility')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranking_utils import elo_ratings_func, evaluate_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What was the initial goal of OpenAI?: 1542.6604952925863\n",
      "Who founded OpenAI?: 1533.3860469059182\n",
      "What did OpenAI release in 2016?: 1512.1017342737223\n",
      "How did the AI agents in OpenAI Five work together?: 1509.283613629824\n",
      "What project did OpenAI showcase in 2018?: 1483.2228811372627\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompts = [\n",
    "            \"Who founded OpenAI?\", \n",
    "            \"What was the initial goal of OpenAI?\",\n",
    "            \"What did OpenAI release in 2016?\", \n",
    "            \"What project did OpenAI showcase in 2018?\",\n",
    "            \"How did the AI agents in OpenAI Five work together?\"\n",
    "                ]\n",
    "elo_ratings = {prompt: 1500 for prompt in prompts}  # Initial ratings\n",
    "\n",
    "# Conduct multiple rounds of evaluation\n",
    "for _ in range(10):  # Number of rounds\n",
    "    elo_ratings = elo_ratings_func(prompts, elo_ratings)\n",
    "\n",
    "# Sort prompts by their final Elo ratings\n",
    "sorted_prompts = sorted(prompts, key=lambda x: elo_ratings[x], reverse=True)\n",
    "\n",
    "# Print the ranked prompts\n",
    "for prompt in sorted_prompts:\n",
    "    print(f\"{prompt}: {elo_ratings[prompt]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'main_prompt': {'Monte Carlo Evaluation': 2.0, 'Elo Rating Evaluation': 1489.2019499940866}, 'test_case_1': {'Monte Carlo Evaluation': 2.0, 'Elo Rating Evaluation': 1504.2019499940866}, 'test_case_2': {'Monte Carlo Evaluation': 1.98, 'Elo Rating Evaluation': 1504.2019499940866}, 'test_case_3': {'Monte Carlo Evaluation': 1.99, 'Elo Rating Evaluation': 1504.2019499940866}, 'test_case_4': {'Monte Carlo Evaluation': 1.97, 'Elo Rating Evaluation': 1519.2019499940866}, 'test_case_5': {'Monte Carlo Evaluation': 2.03, 'Elo Rating Evaluation': 1519.2019499940866}}\n"
     ]
    }
   ],
   "source": [
    "main_prompt = \"why we use OepenAI?\"\n",
    "test_cases = [\"Who founded OpenAI?\", \n",
    "                \"What was the initial goal of OpenAI?\",\n",
    "                \"What did OpenAI release in 2016?\", \n",
    "                \"What project did OpenAI showcase in 2018?\",\n",
    "                \"How did the AI agents in OpenAI Five work together?\"\n",
    "                ]\n",
    "result = evaluate_prompt(main_prompt, test_cases)\n",
    "print(result)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tenx_week6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
